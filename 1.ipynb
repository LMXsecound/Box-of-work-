{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "# 下载训练集\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "# 下载测试集\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# show image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(numpy.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow( next(iter(train_loader))[0][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uisng GPU\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "conv1.weight \t torch.Size([10, 1, 5, 5])\n",
      "conv1.bias \t torch.Size([10])\n",
      "conv2.weight \t torch.Size([20, 10, 5, 5])\n",
      "conv2.bias \t torch.Size([20])\n",
      "fc1.weight \t torch.Size([50, 320])\n",
      "fc1.bias \t torch.Size([50])\n",
      "fc2.weight \t torch.Size([10, 50])\n",
      "fc2.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# show model size in each layer\n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, '\\t', param.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dandelion\\AppData\\Local\\Temp\\ipykernel_31408\\3974891827.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.323725\n",
      "Train Epoch: 1 [1536/60000 (3%)]\tLoss: 1.966425\n",
      "Train Epoch: 1 [3072/60000 (5%)]\tLoss: 1.114956\n",
      "Train Epoch: 1 [4608/60000 (8%)]\tLoss: 0.982850\n",
      "Train Epoch: 1 [6144/60000 (10%)]\tLoss: 0.654270\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.902084\n",
      "Train Epoch: 1 [9216/60000 (15%)]\tLoss: 0.684577\n",
      "Train Epoch: 1 [10752/60000 (18%)]\tLoss: 0.655656\n",
      "Train Epoch: 1 [12288/60000 (20%)]\tLoss: 0.793310\n",
      "Train Epoch: 1 [13824/60000 (23%)]\tLoss: 0.533346\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.713115\n",
      "Train Epoch: 1 [16896/60000 (28%)]\tLoss: 0.445419\n",
      "Train Epoch: 1 [18432/60000 (31%)]\tLoss: 0.662628\n",
      "Train Epoch: 1 [19968/60000 (33%)]\tLoss: 0.821292\n",
      "Train Epoch: 1 [21504/60000 (36%)]\tLoss: 0.348947\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.398657\n",
      "Train Epoch: 1 [24576/60000 (41%)]\tLoss: 0.573675\n",
      "Train Epoch: 1 [26112/60000 (43%)]\tLoss: 0.332475\n",
      "Train Epoch: 1 [27648/60000 (46%)]\tLoss: 0.337762\n",
      "Train Epoch: 1 [29184/60000 (49%)]\tLoss: 0.544432\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.503642\n",
      "Train Epoch: 1 [32256/60000 (54%)]\tLoss: 0.559650\n",
      "Train Epoch: 1 [33792/60000 (56%)]\tLoss: 0.222691\n",
      "Train Epoch: 1 [35328/60000 (59%)]\tLoss: 0.211244\n",
      "Train Epoch: 1 [36864/60000 (61%)]\tLoss: 0.379426\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.281611\n",
      "Train Epoch: 1 [39936/60000 (67%)]\tLoss: 0.430911\n",
      "Train Epoch: 1 [41472/60000 (69%)]\tLoss: 0.419068\n",
      "Train Epoch: 1 [43008/60000 (72%)]\tLoss: 0.273335\n",
      "Train Epoch: 1 [44544/60000 (74%)]\tLoss: 0.308585\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.368150\n",
      "Train Epoch: 1 [47616/60000 (79%)]\tLoss: 0.371741\n",
      "Train Epoch: 1 [49152/60000 (82%)]\tLoss: 0.579349\n",
      "Train Epoch: 1 [50688/60000 (84%)]\tLoss: 0.458234\n",
      "Train Epoch: 1 [52224/60000 (87%)]\tLoss: 0.364442\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.242092\n",
      "Train Epoch: 1 [55296/60000 (92%)]\tLoss: 0.449283\n",
      "Train Epoch: 1 [56832/60000 (95%)]\tLoss: 0.356336\n",
      "Train Epoch: 1 [58368/60000 (97%)]\tLoss: 0.266148\n",
      "Train Epoch: 1 [59904/60000 (100%)]\tLoss: 0.508374\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.191355\n",
      "Train Epoch: 2 [1536/60000 (3%)]\tLoss: 0.283302\n",
      "Train Epoch: 2 [3072/60000 (5%)]\tLoss: 0.215060\n",
      "Train Epoch: 2 [4608/60000 (8%)]\tLoss: 0.329497\n",
      "Train Epoch: 2 [6144/60000 (10%)]\tLoss: 0.240113\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.297757\n",
      "Train Epoch: 2 [9216/60000 (15%)]\tLoss: 0.298191\n",
      "Train Epoch: 2 [10752/60000 (18%)]\tLoss: 0.174392\n",
      "Train Epoch: 2 [12288/60000 (20%)]\tLoss: 0.261503\n",
      "Train Epoch: 2 [13824/60000 (23%)]\tLoss: 0.165683\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.229366\n",
      "Train Epoch: 2 [16896/60000 (28%)]\tLoss: 0.259108\n",
      "Train Epoch: 2 [18432/60000 (31%)]\tLoss: 0.330337\n",
      "Train Epoch: 2 [19968/60000 (33%)]\tLoss: 0.345741\n",
      "Train Epoch: 2 [21504/60000 (36%)]\tLoss: 0.188208\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.380531\n",
      "Train Epoch: 2 [24576/60000 (41%)]\tLoss: 0.592043\n",
      "Train Epoch: 2 [26112/60000 (43%)]\tLoss: 0.159971\n",
      "Train Epoch: 2 [27648/60000 (46%)]\tLoss: 0.332432\n",
      "Train Epoch: 2 [29184/60000 (49%)]\tLoss: 0.318819\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.252695\n",
      "Train Epoch: 2 [32256/60000 (54%)]\tLoss: 0.228588\n",
      "Train Epoch: 2 [33792/60000 (56%)]\tLoss: 0.171934\n",
      "Train Epoch: 2 [35328/60000 (59%)]\tLoss: 0.152929\n",
      "Train Epoch: 2 [36864/60000 (61%)]\tLoss: 0.283914\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.352667\n",
      "Train Epoch: 2 [39936/60000 (67%)]\tLoss: 0.382944\n",
      "Train Epoch: 2 [41472/60000 (69%)]\tLoss: 0.244197\n",
      "Train Epoch: 2 [43008/60000 (72%)]\tLoss: 0.343974\n",
      "Train Epoch: 2 [44544/60000 (74%)]\tLoss: 0.103942\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.244887\n",
      "Train Epoch: 2 [47616/60000 (79%)]\tLoss: 0.140864\n",
      "Train Epoch: 2 [49152/60000 (82%)]\tLoss: 0.623350\n",
      "Train Epoch: 2 [50688/60000 (84%)]\tLoss: 0.408412\n",
      "Train Epoch: 2 [52224/60000 (87%)]\tLoss: 0.208468\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.196257\n",
      "Train Epoch: 2 [55296/60000 (92%)]\tLoss: 0.262018\n",
      "Train Epoch: 2 [56832/60000 (95%)]\tLoss: 0.298123\n",
      "Train Epoch: 2 [58368/60000 (97%)]\tLoss: 0.141953\n",
      "Train Epoch: 2 [59904/60000 (100%)]\tLoss: 0.528085\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.177698\n",
      "Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.140123\n",
      "Train Epoch: 3 [3072/60000 (5%)]\tLoss: 0.109408\n",
      "Train Epoch: 3 [4608/60000 (8%)]\tLoss: 0.174715\n",
      "Train Epoch: 3 [6144/60000 (10%)]\tLoss: 0.143139\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.303781\n",
      "Train Epoch: 3 [9216/60000 (15%)]\tLoss: 0.296461\n",
      "Train Epoch: 3 [10752/60000 (18%)]\tLoss: 0.102918\n",
      "Train Epoch: 3 [12288/60000 (20%)]\tLoss: 0.132120\n",
      "Train Epoch: 3 [13824/60000 (23%)]\tLoss: 0.245668\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.234814\n",
      "Train Epoch: 3 [16896/60000 (28%)]\tLoss: 0.241030\n",
      "Train Epoch: 3 [18432/60000 (31%)]\tLoss: 0.159434\n",
      "Train Epoch: 3 [19968/60000 (33%)]\tLoss: 0.420383\n",
      "Train Epoch: 3 [21504/60000 (36%)]\tLoss: 0.176152\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.313035\n",
      "Train Epoch: 3 [24576/60000 (41%)]\tLoss: 0.308067\n",
      "Train Epoch: 3 [26112/60000 (43%)]\tLoss: 0.268096\n",
      "Train Epoch: 3 [27648/60000 (46%)]\tLoss: 0.231148\n",
      "Train Epoch: 3 [29184/60000 (49%)]\tLoss: 0.300936\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.236685\n",
      "Train Epoch: 3 [32256/60000 (54%)]\tLoss: 0.223774\n",
      "Train Epoch: 3 [33792/60000 (56%)]\tLoss: 0.186187\n",
      "Train Epoch: 3 [35328/60000 (59%)]\tLoss: 0.171629\n",
      "Train Epoch: 3 [36864/60000 (61%)]\tLoss: 0.180063\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.170818\n",
      "Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.188748\n",
      "Train Epoch: 3 [41472/60000 (69%)]\tLoss: 0.318073\n",
      "Train Epoch: 3 [43008/60000 (72%)]\tLoss: 0.446026\n",
      "Train Epoch: 3 [44544/60000 (74%)]\tLoss: 0.068394\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.256816\n",
      "Train Epoch: 3 [47616/60000 (79%)]\tLoss: 0.161446\n",
      "Train Epoch: 3 [49152/60000 (82%)]\tLoss: 0.346746\n",
      "Train Epoch: 3 [50688/60000 (84%)]\tLoss: 0.228390\n",
      "Train Epoch: 3 [52224/60000 (87%)]\tLoss: 0.236497\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.313535\n",
      "Train Epoch: 3 [55296/60000 (92%)]\tLoss: 0.225166\n",
      "Train Epoch: 3 [56832/60000 (95%)]\tLoss: 0.299059\n",
      "Train Epoch: 3 [58368/60000 (97%)]\tLoss: 0.095116\n",
      "Train Epoch: 3 [59904/60000 (100%)]\tLoss: 0.708786\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.163258\n",
      "Train Epoch: 4 [1536/60000 (3%)]\tLoss: 0.315711\n",
      "Train Epoch: 4 [3072/60000 (5%)]\tLoss: 0.100843\n",
      "Train Epoch: 4 [4608/60000 (8%)]\tLoss: 0.159978\n",
      "Train Epoch: 4 [6144/60000 (10%)]\tLoss: 0.176425\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.107997\n",
      "Train Epoch: 4 [9216/60000 (15%)]\tLoss: 0.188007\n",
      "Train Epoch: 4 [10752/60000 (18%)]\tLoss: 0.289173\n",
      "Train Epoch: 4 [12288/60000 (20%)]\tLoss: 0.232330\n",
      "Train Epoch: 4 [13824/60000 (23%)]\tLoss: 0.231952\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.177740\n",
      "Train Epoch: 4 [16896/60000 (28%)]\tLoss: 0.203703\n",
      "Train Epoch: 4 [18432/60000 (31%)]\tLoss: 0.161177\n",
      "Train Epoch: 4 [19968/60000 (33%)]\tLoss: 0.208969\n",
      "Train Epoch: 4 [21504/60000 (36%)]\tLoss: 0.147239\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.315675\n",
      "Train Epoch: 4 [24576/60000 (41%)]\tLoss: 0.259112\n",
      "Train Epoch: 4 [26112/60000 (43%)]\tLoss: 0.136882\n",
      "Train Epoch: 4 [27648/60000 (46%)]\tLoss: 0.258926\n",
      "Train Epoch: 4 [29184/60000 (49%)]\tLoss: 0.185617\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.244622\n",
      "Train Epoch: 4 [32256/60000 (54%)]\tLoss: 0.206918\n",
      "Train Epoch: 4 [33792/60000 (56%)]\tLoss: 0.080612\n",
      "Train Epoch: 4 [35328/60000 (59%)]\tLoss: 0.061186\n",
      "Train Epoch: 4 [36864/60000 (61%)]\tLoss: 0.117091\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.220261\n",
      "Train Epoch: 4 [39936/60000 (67%)]\tLoss: 0.082453\n",
      "Train Epoch: 4 [41472/60000 (69%)]\tLoss: 0.156749\n",
      "Train Epoch: 4 [43008/60000 (72%)]\tLoss: 0.235781\n",
      "Train Epoch: 4 [44544/60000 (74%)]\tLoss: 0.075416\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.197558\n",
      "Train Epoch: 4 [47616/60000 (79%)]\tLoss: 0.106343\n",
      "Train Epoch: 4 [49152/60000 (82%)]\tLoss: 0.224568\n",
      "Train Epoch: 4 [50688/60000 (84%)]\tLoss: 0.231255\n",
      "Train Epoch: 4 [52224/60000 (87%)]\tLoss: 0.232800\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.201982\n",
      "Train Epoch: 4 [55296/60000 (92%)]\tLoss: 0.248462\n",
      "Train Epoch: 4 [56832/60000 (95%)]\tLoss: 0.128927\n",
      "Train Epoch: 4 [58368/60000 (97%)]\tLoss: 0.047951\n",
      "Train Epoch: 4 [59904/60000 (100%)]\tLoss: 0.337944\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.071863\n",
      "Train Epoch: 5 [1536/60000 (3%)]\tLoss: 0.104985\n",
      "Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.106324\n",
      "Train Epoch: 5 [4608/60000 (8%)]\tLoss: 0.233188\n",
      "Train Epoch: 5 [6144/60000 (10%)]\tLoss: 0.114922\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.112974\n",
      "Train Epoch: 5 [9216/60000 (15%)]\tLoss: 0.273237\n",
      "Train Epoch: 5 [10752/60000 (18%)]\tLoss: 0.172489\n",
      "Train Epoch: 5 [12288/60000 (20%)]\tLoss: 0.168525\n",
      "Train Epoch: 5 [13824/60000 (23%)]\tLoss: 0.234406\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.069985\n",
      "Train Epoch: 5 [16896/60000 (28%)]\tLoss: 0.120847\n",
      "Train Epoch: 5 [18432/60000 (31%)]\tLoss: 0.157897\n",
      "Train Epoch: 5 [19968/60000 (33%)]\tLoss: 0.177914\n",
      "Train Epoch: 5 [21504/60000 (36%)]\tLoss: 0.152377\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.199073\n",
      "Train Epoch: 5 [24576/60000 (41%)]\tLoss: 0.441040\n",
      "Train Epoch: 5 [26112/60000 (43%)]\tLoss: 0.203027\n",
      "Train Epoch: 5 [27648/60000 (46%)]\tLoss: 0.190259\n",
      "Train Epoch: 5 [29184/60000 (49%)]\tLoss: 0.169705\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.265690\n",
      "Train Epoch: 5 [32256/60000 (54%)]\tLoss: 0.233015\n",
      "Train Epoch: 5 [33792/60000 (56%)]\tLoss: 0.092228\n",
      "Train Epoch: 5 [35328/60000 (59%)]\tLoss: 0.067024\n",
      "Train Epoch: 5 [36864/60000 (61%)]\tLoss: 0.053124\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.110051\n",
      "Train Epoch: 5 [39936/60000 (67%)]\tLoss: 0.152921\n",
      "Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.118728\n",
      "Train Epoch: 5 [43008/60000 (72%)]\tLoss: 0.138767\n",
      "Train Epoch: 5 [44544/60000 (74%)]\tLoss: 0.184534\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.265980\n",
      "Train Epoch: 5 [47616/60000 (79%)]\tLoss: 0.203271\n",
      "Train Epoch: 5 [49152/60000 (82%)]\tLoss: 0.213174\n",
      "Train Epoch: 5 [50688/60000 (84%)]\tLoss: 0.340112\n",
      "Train Epoch: 5 [52224/60000 (87%)]\tLoss: 0.127541\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.081681\n",
      "Train Epoch: 5 [55296/60000 (92%)]\tLoss: 0.147216\n",
      "Train Epoch: 5 [56832/60000 (95%)]\tLoss: 0.241582\n",
      "Train Epoch: 5 [58368/60000 (97%)]\tLoss: 0.032935\n",
      "Train Epoch: 5 [59904/60000 (100%)]\tLoss: 0.460263\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.143084\n",
      "Train Epoch: 6 [1536/60000 (3%)]\tLoss: 0.247748\n",
      "Train Epoch: 6 [3072/60000 (5%)]\tLoss: 0.101453\n",
      "Train Epoch: 6 [4608/60000 (8%)]\tLoss: 0.372312\n",
      "Train Epoch: 6 [6144/60000 (10%)]\tLoss: 0.123632\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.118270\n",
      "Train Epoch: 6 [9216/60000 (15%)]\tLoss: 0.118046\n",
      "Train Epoch: 6 [10752/60000 (18%)]\tLoss: 0.196735\n",
      "Train Epoch: 6 [12288/60000 (20%)]\tLoss: 0.160908\n",
      "Train Epoch: 6 [13824/60000 (23%)]\tLoss: 0.103680\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.159112\n",
      "Train Epoch: 6 [16896/60000 (28%)]\tLoss: 0.167280\n",
      "Train Epoch: 6 [18432/60000 (31%)]\tLoss: 0.156922\n",
      "Train Epoch: 6 [19968/60000 (33%)]\tLoss: 0.396526\n",
      "Train Epoch: 6 [21504/60000 (36%)]\tLoss: 0.116302\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.161655\n",
      "Train Epoch: 6 [24576/60000 (41%)]\tLoss: 0.392930\n",
      "Train Epoch: 6 [26112/60000 (43%)]\tLoss: 0.078814\n",
      "Train Epoch: 6 [27648/60000 (46%)]\tLoss: 0.137525\n",
      "Train Epoch: 6 [29184/60000 (49%)]\tLoss: 0.200445\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.154604\n",
      "Train Epoch: 6 [32256/60000 (54%)]\tLoss: 0.112681\n",
      "Train Epoch: 6 [33792/60000 (56%)]\tLoss: 0.057871\n",
      "Train Epoch: 6 [35328/60000 (59%)]\tLoss: 0.078477\n",
      "Train Epoch: 6 [36864/60000 (61%)]\tLoss: 0.105191\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.133426\n",
      "Train Epoch: 6 [39936/60000 (67%)]\tLoss: 0.130307\n",
      "Train Epoch: 6 [41472/60000 (69%)]\tLoss: 0.240685\n",
      "Train Epoch: 6 [43008/60000 (72%)]\tLoss: 0.111361\n",
      "Train Epoch: 6 [44544/60000 (74%)]\tLoss: 0.140227\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.191253\n",
      "Train Epoch: 6 [47616/60000 (79%)]\tLoss: 0.071976\n",
      "Train Epoch: 6 [49152/60000 (82%)]\tLoss: 0.145873\n",
      "Train Epoch: 6 [50688/60000 (84%)]\tLoss: 0.203268\n",
      "Train Epoch: 6 [52224/60000 (87%)]\tLoss: 0.205501\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.187982\n",
      "Train Epoch: 6 [55296/60000 (92%)]\tLoss: 0.206008\n",
      "Train Epoch: 6 [56832/60000 (95%)]\tLoss: 0.193014\n",
      "Train Epoch: 6 [58368/60000 (97%)]\tLoss: 0.168488\n",
      "Train Epoch: 6 [59904/60000 (100%)]\tLoss: 0.229051\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.146921\n",
      "Train Epoch: 7 [1536/60000 (3%)]\tLoss: 0.197547\n",
      "Train Epoch: 7 [3072/60000 (5%)]\tLoss: 0.101141\n",
      "Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.280278\n",
      "Train Epoch: 7 [6144/60000 (10%)]\tLoss: 0.045398\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.135768\n",
      "Train Epoch: 7 [9216/60000 (15%)]\tLoss: 0.206824\n",
      "Train Epoch: 7 [10752/60000 (18%)]\tLoss: 0.142681\n",
      "Train Epoch: 7 [12288/60000 (20%)]\tLoss: 0.122424\n",
      "Train Epoch: 7 [13824/60000 (23%)]\tLoss: 0.209774\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.135017\n",
      "Train Epoch: 7 [16896/60000 (28%)]\tLoss: 0.096445\n",
      "Train Epoch: 7 [18432/60000 (31%)]\tLoss: 0.129081\n",
      "Train Epoch: 7 [19968/60000 (33%)]\tLoss: 0.208769\n",
      "Train Epoch: 7 [21504/60000 (36%)]\tLoss: 0.137612\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.164547\n",
      "Train Epoch: 7 [24576/60000 (41%)]\tLoss: 0.175318\n",
      "Train Epoch: 7 [26112/60000 (43%)]\tLoss: 0.034511\n",
      "Train Epoch: 7 [27648/60000 (46%)]\tLoss: 0.198639\n",
      "Train Epoch: 7 [29184/60000 (49%)]\tLoss: 0.083832\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.073060\n",
      "Train Epoch: 7 [32256/60000 (54%)]\tLoss: 0.176325\n",
      "Train Epoch: 7 [33792/60000 (56%)]\tLoss: 0.088789\n",
      "Train Epoch: 7 [35328/60000 (59%)]\tLoss: 0.064003\n",
      "Train Epoch: 7 [36864/60000 (61%)]\tLoss: 0.082858\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.091954\n",
      "Train Epoch: 7 [39936/60000 (67%)]\tLoss: 0.108833\n",
      "Train Epoch: 7 [41472/60000 (69%)]\tLoss: 0.138223\n",
      "Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.161923\n",
      "Train Epoch: 7 [44544/60000 (74%)]\tLoss: 0.051840\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.120235\n",
      "Train Epoch: 7 [47616/60000 (79%)]\tLoss: 0.155705\n",
      "Train Epoch: 7 [49152/60000 (82%)]\tLoss: 0.351665\n",
      "Train Epoch: 7 [50688/60000 (84%)]\tLoss: 0.182618\n",
      "Train Epoch: 7 [52224/60000 (87%)]\tLoss: 0.208954\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.051405\n",
      "Train Epoch: 7 [55296/60000 (92%)]\tLoss: 0.170477\n",
      "Train Epoch: 7 [56832/60000 (95%)]\tLoss: 0.322184\n",
      "Train Epoch: 7 [58368/60000 (97%)]\tLoss: 0.041695\n",
      "Train Epoch: 7 [59904/60000 (100%)]\tLoss: 0.398448\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.137509\n",
      "Train Epoch: 8 [1536/60000 (3%)]\tLoss: 0.120074\n",
      "Train Epoch: 8 [3072/60000 (5%)]\tLoss: 0.027669\n",
      "Train Epoch: 8 [4608/60000 (8%)]\tLoss: 0.197094\n",
      "Train Epoch: 8 [6144/60000 (10%)]\tLoss: 0.080891\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.129191\n",
      "Train Epoch: 8 [9216/60000 (15%)]\tLoss: 0.127730\n",
      "Train Epoch: 8 [10752/60000 (18%)]\tLoss: 0.189414\n",
      "Train Epoch: 8 [12288/60000 (20%)]\tLoss: 0.174036\n",
      "Train Epoch: 8 [13824/60000 (23%)]\tLoss: 0.132108\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.252412\n",
      "Train Epoch: 8 [16896/60000 (28%)]\tLoss: 0.113931\n",
      "Train Epoch: 8 [18432/60000 (31%)]\tLoss: 0.027136\n",
      "Train Epoch: 8 [19968/60000 (33%)]\tLoss: 0.235631\n",
      "Train Epoch: 8 [21504/60000 (36%)]\tLoss: 0.068023\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.199449\n",
      "Train Epoch: 8 [24576/60000 (41%)]\tLoss: 0.264871\n",
      "Train Epoch: 8 [26112/60000 (43%)]\tLoss: 0.055856\n",
      "Train Epoch: 8 [27648/60000 (46%)]\tLoss: 0.290822\n",
      "Train Epoch: 8 [29184/60000 (49%)]\tLoss: 0.088732\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.210154\n",
      "Train Epoch: 8 [32256/60000 (54%)]\tLoss: 0.178231\n",
      "Train Epoch: 8 [33792/60000 (56%)]\tLoss: 0.032199\n",
      "Train Epoch: 8 [35328/60000 (59%)]\tLoss: 0.067061\n",
      "Train Epoch: 8 [36864/60000 (61%)]\tLoss: 0.045196\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.065189\n",
      "Train Epoch: 8 [39936/60000 (67%)]\tLoss: 0.163148\n",
      "Train Epoch: 8 [41472/60000 (69%)]\tLoss: 0.087067\n",
      "Train Epoch: 8 [43008/60000 (72%)]\tLoss: 0.188114\n",
      "Train Epoch: 8 [44544/60000 (74%)]\tLoss: 0.033770\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.229490\n",
      "Train Epoch: 8 [47616/60000 (79%)]\tLoss: 0.074370\n",
      "Train Epoch: 8 [49152/60000 (82%)]\tLoss: 0.241770\n",
      "Train Epoch: 8 [50688/60000 (84%)]\tLoss: 0.181685\n",
      "Train Epoch: 8 [52224/60000 (87%)]\tLoss: 0.167856\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.097521\n",
      "Train Epoch: 8 [55296/60000 (92%)]\tLoss: 0.219112\n",
      "Train Epoch: 8 [56832/60000 (95%)]\tLoss: 0.353295\n",
      "Train Epoch: 8 [58368/60000 (97%)]\tLoss: 0.164139\n",
      "Train Epoch: 8 [59904/60000 (100%)]\tLoss: 0.345738\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.095216\n",
      "Train Epoch: 9 [1536/60000 (3%)]\tLoss: 0.166967\n",
      "Train Epoch: 9 [3072/60000 (5%)]\tLoss: 0.117684\n",
      "Train Epoch: 9 [4608/60000 (8%)]\tLoss: 0.242396\n",
      "Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.145145\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.106872\n",
      "Train Epoch: 9 [9216/60000 (15%)]\tLoss: 0.184663\n",
      "Train Epoch: 9 [10752/60000 (18%)]\tLoss: 0.154203\n",
      "Train Epoch: 9 [12288/60000 (20%)]\tLoss: 0.100057\n",
      "Train Epoch: 9 [13824/60000 (23%)]\tLoss: 0.311093\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.170527\n",
      "Train Epoch: 9 [16896/60000 (28%)]\tLoss: 0.092017\n",
      "Train Epoch: 9 [18432/60000 (31%)]\tLoss: 0.177971\n",
      "Train Epoch: 9 [19968/60000 (33%)]\tLoss: 0.221595\n",
      "Train Epoch: 9 [21504/60000 (36%)]\tLoss: 0.048215\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.329319\n",
      "Train Epoch: 9 [24576/60000 (41%)]\tLoss: 0.379774\n",
      "Train Epoch: 9 [26112/60000 (43%)]\tLoss: 0.072444\n",
      "Train Epoch: 9 [27648/60000 (46%)]\tLoss: 0.098272\n",
      "Train Epoch: 9 [29184/60000 (49%)]\tLoss: 0.185897\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.156842\n",
      "Train Epoch: 9 [32256/60000 (54%)]\tLoss: 0.018788\n",
      "Train Epoch: 9 [33792/60000 (56%)]\tLoss: 0.050397\n",
      "Train Epoch: 9 [35328/60000 (59%)]\tLoss: 0.120965\n",
      "Train Epoch: 9 [36864/60000 (61%)]\tLoss: 0.086202\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.121105\n",
      "Train Epoch: 9 [39936/60000 (67%)]\tLoss: 0.089606\n",
      "Train Epoch: 9 [41472/60000 (69%)]\tLoss: 0.101587\n",
      "Train Epoch: 9 [43008/60000 (72%)]\tLoss: 0.067654\n",
      "Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.085919\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.232191\n",
      "Train Epoch: 9 [47616/60000 (79%)]\tLoss: 0.164509\n",
      "Train Epoch: 9 [49152/60000 (82%)]\tLoss: 0.255389\n",
      "Train Epoch: 9 [50688/60000 (84%)]\tLoss: 0.320945\n",
      "Train Epoch: 9 [52224/60000 (87%)]\tLoss: 0.265739\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.265329\n",
      "Train Epoch: 9 [55296/60000 (92%)]\tLoss: 0.230075\n",
      "Train Epoch: 9 [56832/60000 (95%)]\tLoss: 0.285770\n",
      "Train Epoch: 9 [58368/60000 (97%)]\tLoss: 0.029175\n",
      "Train Epoch: 9 [59904/60000 (100%)]\tLoss: 0.311083\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 24 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dandelion\\AppData\\Local\\Temp\\ipykernel_31408\\3974891827.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0442, Accuracy: 9860/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data.to(device))\n",
    "        test_loss += F.nll_loss(output, target.to(device), size_average=False).item()\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.to(device).view_as(pred)).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
